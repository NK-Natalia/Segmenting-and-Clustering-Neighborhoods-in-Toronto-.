{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methodology\n",
    "\n",
    " 1. Get the information about Post Code of the neighborhoods of Toronto from wikipage: https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M,\n",
    " 2. Use BeautifulSoup for clean information and some standard features like .drop, .groupby, .replace for dataframe formation.\n",
    " 3. .read_csv  from http://cocl.us/Geospatial_data to get the latitude and longitude values of the neighborhoods of Toronto.\n",
    " 4. Use geopy library to get the latitude and longitude values of Toronto. The geographical coordinates of Toronto are 43.653963 , -79.387207.\n",
    " 5. Create map of Toronto using latitude and longitude values used .folium and add markers to map by .CircleMarker features.\n",
    " 6. Create URL by Foursquare API\n",
    " 7. Create a function to get all venues for each neighborhood and create a dataframe of them.\n",
    " 8. Group the venues across all neighborhoods of toronto by .groupby and .count features.\n",
    " 9. Count number of uniques categories and create a list of them by .len, .unique, .sort_values, .groupby and .count.\n",
    " 10. Create a file with dummy values by venue category (.get_dummies).\n",
    " 11. .Drop some venues categories from the data, witch irrelevant ('Airport', 'Rental Car Location', 'Bike Shop', 'Gay Bar', 'Strip Club', 'Nightclub', 'Electronics Store', 'Airport Food Court', 'Airport Gate', 'Airport Lounge', 'Airport Service', 'Airport Terminal')\n",
    " 12. Add column Total in dataframe (.sum features).\n",
    " 13. Determine the optimal value of K for our dataset using the Silhouette Coefficient Method. n_clusters=2 has highest Silhouette Coefficient. This means that 2 should be the optimal number of clusters.\n",
    " For n_clusters=2, The Silhouette Coefficient is 0.7895622910021808.\n",
    " 14. Run k-means to cluster the neighborhood into 2 clusters. The Total and Total Sum of cluster 0 has smallest value. It shows that the market is not saturated.\n",
    " 15. Create Dataframe with Neighborhood,Cluster No and Total Sum.\n",
    " 16. Create Clusters Map with .folium.\n",
    " 17. Identify categories of potential competitors .loc and check each using data .foursquare.\n",
    " 18. Using .Marker mark competitors' positions on the map.\n",
    " 19. Import statistics library and count the coordinates of the center of the area defined by clustering (.mean).\n",
    " 20. Build the Map with .folium and with .CircleMarker mark the area most advantageous for opening a store.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Results\n",
    "\n",
    " From this venues data it was filtered and used only study relevant data. During the study, the area of the most favorable store location was determined, taking into account the location of competitors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Discussion\n",
    "\n",
    " When choosing a place to open a store, it is worth remembering the location of competitor stores. You should also pay attention to the area marked on the map with red circles. During the clustering, it was found that the saturation of the market with various venues is higher, therefore, their consideration should be excluded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Conclusion\n",
    " \n",
    " After the completion of this project, we have a small example of what real research projects look like. I used some commonly used python libraries to collect web data, used the Foursquare API to explore areas of Toronto, and saw the results of area segmentation using the Folium map. The potential for such an analysis in a real business problem is discussed in great detail. Finally, since my analysis was mainly focused on the possibilities of opening a customer-oriented needlework store, regardless of social factors, the model turned out to be simple and clear. For other types of business, of course, you should complicate the model and introduce more criteria for analysis. Hopefully this type of analysis will provide you with an initial guide to more real-world data science challenges.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
